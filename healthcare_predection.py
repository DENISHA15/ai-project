# -*- coding: utf-8 -*-
"""healthcare_predection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V3biRjdS441KZiH_9JvaPXcsPUeFKsDV
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
import pickle

df = pd.read_csv("/content/healthcare_dataset.csv")

df.drop(columns=['Name'], inplace=True)

df

date_columns = ['Date of Admission', 'Discharge Date']
for col in date_columns:
    df[col] = pd.to_datetime(df[col], errors='coerce')
    df[col] = df[col].map(lambda x: x.toordinal() if pd.notnull(x) else np.nan)

df.fillna(df.median(numeric_only=True), inplace=True)

df.fillna(df.mode().iloc[0], inplace=True)

label_encoders = {}
categorical_cols = df.select_dtypes(include='object').columns

for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
    label_encoders[col] = le

X = df.drop(columns=['Medical Condition'])
y = df['Medical Condition']

X

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_test

model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

y_pred

a=[2, 3, 5, ..., 3, 3, 2]
len(a)

y_test

accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

comparison = pd.DataFrame({
    'Actual': y_test.values,
    'Predicted': y_pred
})

print("Comparison:\n", comparison.head(10))

print("Accuracy: {:.2f}%".format(accuracy * 100))

with open("healthcare_logistic_model.pkl", "wb") as f:
    pickle.dump((model, X.columns.tolist()), f)

print("Model saved as 'healthcare_logistic_model.pkl'")

!pip install lime

import lime
import lime.lime_tabular
import numpy as np

# Create LIME explainer
explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train),
    feature_names=X_train.columns.tolist(),
    class_names=label_encoders['Medical Condition'].classes_.tolist(),  # class names from LabelEncoder for 'Medical Condition'
    mode='classification'
)

# Helper: Keras predict_proba wrapper
# Logistic Regression model directly provides predict_proba
def predict_proba_wrapper(data):
    return model.predict_proba(data)

# Explain a test instance (e.g., first row)
i = 0
exp = explainer.explain_instance(
    data_row=X_test.iloc[i].values, # Pass data_row as a numpy array
    predict_fn=predict_proba_wrapper,
    num_features=5
)

# Show explanation in notebook
exp.show_in_notebook(show_table=True)

import matplotlib.pyplot as plt

# For Logistic Regression, use absolute coefficients as a measure of importance
importances = np.abs(model.coef_[0])  # Use coef_[0] for binary classification, adjust for multi-class
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10,6))
plt.title("Feature Importances (Absolute Coefficients)")
plt.bar(range(X_train.shape[1]), importances[indices], align="center")
plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=45, ha='right')
plt.tight_layout()
plt.show()

!pip install eli5

import eli5
from eli5.sklearn import PermutationImportance

perm = PermutationImportance(model, random_state=42).fit(X_test, y_test)
eli5.show_weights(perm, feature_names=X_test.columns.tolist())

!pip install alibi

import numpy as np
import matplotlib.pyplot as plt
from alibi.explainers import ALE
from sklearn.preprocessing import LabelEncoder, StandardScaler
import pandas as pd # Import pandas to use .toordinal()

# Re-initialize scaler and label_encoders (since they were not saved with the model)
# This assumes the same preprocessing steps are applied in the same order
scaler = StandardScaler()
# Assuming X is still available from previous cells
scaler.fit(X)

label_encoders = {}
categorical_cols_to_encode = ['Gender', 'Blood Type', 'Medical Condition', 'Doctor', 'Hospital', 'Insurance Provider', 'Admission Type', 'Medication', 'Test Results'] # Specify the categorical columns that were encoded

for col in categorical_cols_to_encode:
    le = LabelEncoder()
    # Assuming df is still available from previous cells
    le.fit(df[col].astype(str)) # Fit on the original data to ensure all categories are captured
    label_encoders[col] = le

# Define feature_names using the columns from X_train
feature_names = X_train.columns.tolist()

# Get the target encoder
target_encoder = label_encoders['Medical Condition']


# === Your predict function (after scaling) ===
def predict_fn(X):
    # Convert ordinal dates back to datetime objects for the scaler if necessary, or scale ordinal values directly
    # Given the previous data wrangling, the dates are already ordinals, so direct scaling of the numpy array is appropriate.
    X_scaled = scaler.transform(X)
    return model.predict_proba(X_scaled)

# === Select the feature to analyze ===
target_feature = 'Doctor'  # You can change to 'Hospital', 'Age', etc.
if target_feature in feature_names:
    feature_idx = feature_names.index(target_feature)
else:
    raise ValueError(f"Target feature '{target_feature}' not found in feature names.")


# === Setup ALE explainer ===
# The ALE explainer needs the original training data, not scaled
ale = ALE(predict_fn, feature_names=feature_names)


# === Explain on the test set ===
# Pass X_test as a numpy array
ale_exp = ale.explain(np.array(X_test), features=[feature_idx])

# === Extract class probabilities for a specific class ===
# You can choose a specific class index to plot ALE for
# For example, plotting for the first class in the target encoder
class_index_to_plot = 0 # Change this index to plot for a different medical condition class
if class_index_to_plot < len(target_encoder.classes_):
    ale_class_vals = ale_exp.ale_values[0][:, class_index_to_plot]

    # === Plotting ===
    plt.figure(figsize=(8, 5))
    plt.plot(ale_exp.feature_values[0], ale_class_vals, marker='o')
    plt.xlabel(target_feature)
    plt.ylabel(f'ALE for class {class_index_to_plot} ({target_encoder.classes_[class_index_to_plot]})')
    plt.title(f'ALE Plot for {target_feature} (class {class_index_to_plot})')
    plt.grid(True)
    plt.tight_layout()
    plt.show()
else:
    print(f"Class index {class_index_to_plot} is out of bounds. There are {len(target_encoder.classes_)} classes.")

import pickle

with open("logistic_model.pkl", "wb") as f:
    pickle.dump((model, X.columns.tolist(), scaler, label_encoders), f)

# How many distinct labels did the model actually see?
import numpy as np
np.unique(y_train, return_counts=True)          # should print >2 classes

# What does the model predict on the whole test set?
pred_test = model.predict(X_test)
vals, cnts   = np.unique(pred_test, return_counts=True)
print("Prediction distribution:", dict(zip(vals, cnts)))

from sklearn.linear_model import LogisticRegression
model = LogisticRegression(
    max_iter=2000,
    solver='lbfgs',
    multi_class='multinomial',
    class_weight='balanced'        # fights imbalance
)
model.fit(X_train, y_train)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# ANN model
model = Sequential([
    Dense(16, activation='relu', input_dim=X_train.shape[1]),
    Dense(8, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train model
model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test))

# Evaluate
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy*100:.2f}%")

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder
import numpy as np

# Re-initialize scaler and label_encoders (since they were not saved with the model)
# This assumes the same preprocessing steps are applied in the same order
scaler = StandardScaler()
# Assuming X is still available from previous cells
X_scaled = scaler.fit_transform(X)

# Convert to PyTorch tensors
X_tensor = torch.tensor(X_scaled, dtype=torch.float32)
y_tensor = torch.tensor(y.values, dtype=torch.long) # Use .values to get numpy array and specify dtype

# Split into datasets
X_train_torch, X_test_torch, y_train_torch, y_test_torch = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)
train_ds = TensorDataset(X_train_torch, y_train_torch)
test_ds = TensorDataset(X_test_torch, y_test_torch)
train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)
test_loader = DataLoader(test_ds, batch_size=16, shuffle=False) # Add test loader for evaluation

# Define ANN
class ANN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, output_dim)
        )

    def forward(self, x):
        return self.net(x)

# Get the original class names from the label encoder for the target variable
target_encoder = label_encoders['Medical Condition']
output_dim = len(target_encoder.classes_)


model = ANN(X_train_torch.shape[1], output_dim)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train
epochs = 10
for epoch in range(epochs):
    model.train()
    for xb, yb in train_loader:
        optimizer.zero_grad()
        preds = model(xb)
        loss = criterion(preds, yb)
        loss.backward()
        optimizer.step()
    if (epoch + 1) % 10 == 0:
        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')


# Evaluate
model.eval() # Set model to evaluation mode
with torch.no_grad():
    y_pred_list = []
    y_test_list = []
    for xb, yb in test_loader:
        preds = model(xb)
        y_pred_list.append(torch.argmax(preds, dim=1).cpu().numpy())
        y_test_list.append(yb.cpu().numpy())

    y_pred_torch = np.concatenate(y_pred_list)
    y_test_torch = np.concatenate(y_test_list)


    acc = accuracy_score(y_test_torch, y_pred_torch)
    print("Accuracy (PyTorch):", acc)
    print("Classification Report:\n", classification_report(y_test_torch, y_pred_torch, target_names=target_encoder.classes_))

!pip install gradio

import gradio as gr
import numpy as np
import pandas as pd
import pickle

# Load model, features, scaler, encoders
with open("logistic_model.pkl", "rb") as f:
    model, feature_names, scaler, label_encoders = pickle.load(f)

def predict_healthcare(
    age, gender, blood_type, date_of_admission, doctor, hospital,
    insurance_provider, billing_amount, room_number, admission_type,
    discharge_date, medication, test_results
):
    try:
        input_dict = {
            "Age": age,
            "Gender": label_encoders["Gender"].transform([gender])[0],
            "Blood Type": label_encoders["Blood Type"].transform([blood_type])[0],
            "Date of Admission": pd.to_datetime(date_of_admission).toordinal(),
            "Doctor": label_encoders["Doctor"].transform([doctor])[0],
            "Hospital": label_encoders["Hospital"].transform([hospital])[0],
            "Insurance Provider": label_encoders["Insurance Provider"].transform([insurance_provider])[0],
            "Billing Amount": billing_amount,
            "Room Number": room_number,
            "Admission Type": label_encoders["Admission Type"].transform([admission_type])[0],
            "Discharge Date": pd.to_datetime(discharge_date).toordinal(),
            "Medication": label_encoders["Medication"].transform([medication])[0],
            "Test Results": label_encoders["Test Results"].transform([test_results])[0]
        }

        input_values = np.array([input_dict[feat] for feat in feature_names]).reshape(1, -1)
        input_scaled = scaler.transform(input_values)

        encoded_pred = model.predict(input_scaled)[0]
        medical_condition = label_encoders["Medical Condition"].inverse_transform([encoded_pred])[0]
        return medical_condition

    except Exception as e:
        return f"Error: {str(e)}"

# Dropdown options
gender_choices = label_encoders["Gender"].classes_.tolist()
blood_type_choices = label_encoders["Blood Type"].classes_.tolist()
doctor_choices = label_encoders["Doctor"].classes_.tolist()
hospital_choices = label_encoders["Hospital"].classes_.tolist()
insurance_provider_choices = label_encoders["Insurance Provider"].classes_.tolist()
admission_type_choices = label_encoders["Admission Type"].classes_.tolist()
medication_choices = label_encoders["Medication"].classes_.tolist()
test_results_choices = label_encoders["Test Results"].classes_.tolist()

interface = gr.Interface(
    fn=predict_healthcare,
    inputs=[
        gr.Number(label="Age"),
        gr.Dropdown(choices=gender_choices, label="Gender"),
        gr.Dropdown(choices=blood_type_choices, label="Blood Type"),
        gr.Textbox(label="Date of Admission (YYYY-MM-DD)"),
        gr.Dropdown(choices=doctor_choices, label="Doctor"),
        gr.Dropdown(choices=hospital_choices, label="Hospital"),
        gr.Dropdown(choices=insurance_provider_choices, label="Insurance Provider"),
        gr.Number(label="Billing Amount"),
        gr.Number(label="Room Number"),
        gr.Dropdown(choices=admission_type_choices, label="Admission Type"),
        gr.Textbox(label="Discharge Date (YYYY-MM-DD)"),
        gr.Dropdown(choices=medication_choices, label="Medication"),
        gr.Dropdown(choices=test_results_choices, label="Test Results"),
    ],
    outputs="text",
    title="Healthcare Medical Condition Prediction",
    description="Predicts medical condition using a trained logistic regression model."
)

interface.launch()